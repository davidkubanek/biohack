{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import pdb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "    \n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 12, # 42, ~MAX 20 hours of training\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"scheduler\": 'CosineAnnealingLR',\n",
    "    \"min_lr\": 5e-7,\n",
    "    \"T_max\": 12,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"fold\" : 0,\n",
    "    \"n_fold\": 5,\n",
    "    \"n_accumulate\": 1,\n",
    "    \"device\": get_available_device(),\n",
    "}\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read and split the data\n",
    "## Validation set of 10% of the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_pickle('enveda_chemist_preprocessed.pkl')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df, valid_df = train_test_split(df, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping in Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvedaDataset(Dataset):\n",
    "    def __init__(self, dataframe, labels = ['unable_to_assess', 'close_match', \n",
    "                                            'near_exact_match', 'exact_match']):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): A DataFrame containing 'ground_truth_embeddings', \n",
    "                                       'predicted_embeddings', and output columns.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        \n",
    "        # Convert embeddings to tensors\n",
    "        self.ground_truth_embeddings = torch.tensor(dataframe['ground_truth_embeddings'].tolist(), dtype=torch.float32)\n",
    "        self.predicted_embeddings = torch.tensor(dataframe['predicted_embeddings'].tolist(), dtype=torch.float32)\n",
    "        \n",
    "        # Convert labels to tensor\n",
    "        self.labels = torch.tensor(dataframe[labels].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one sample of data.\"\"\"\n",
    "        return self.ground_truth_embeddings[idx].squeeze(0), self.predicted_embeddings[idx].squeeze(0), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset = EnvedaDataset(dataframe=train_df)\n",
    "validset = EnvedaDataset(dataframe=valid_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping in Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, validloader = DataLoader(trainset, batch_size=CONFIG['train_batch_size']), \\\n",
    "                           DataLoader(validset, batch_size=CONFIG['valid_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gt, pred, labels = next(iter(trainloader))\n",
    "print(gt.shape, pred.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=4):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 16)\n",
    "        self.fc5 = nn.Linear(16 * 2, output_dim)  # Output layer for similarity judgements\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.relu(self.fc4(x))\n",
    "        return x\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        out1 = self.forward_one(input1)\n",
    "        out2 = self.forward_one(input2)\n",
    "        # pdb.set_trace()\n",
    "        # Combine both outputs by concatenation\n",
    "        combined = torch.concat((out1, out2), dim=1) # concatenate embeddings\n",
    "        output = self.fc5(combined)                  # Outputs raw logits\n",
    "        return output\n",
    "    \n",
    "# model = SiameseNetwork()\n",
    "# model(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and validation regime\n",
    "\n",
    "## Training with mixed precision, gradient accumulation, learning with scheduler\n",
    "## Validation logging loss, AUROC, and F1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, criterion, scheduler, dataloader, epoch=CONFIG['epochs']):\n",
    "    model.train()\n",
    "    \n",
    "    scaler = GradScaler()\n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc  = 0.0\n",
    "    running_f1 = 0.0\n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        gt, preds, targets = data\n",
    "        gt, preds, targets = gt.to(CONFIG['device']), preds.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "        batch_size = gt.shape[0]\n",
    "\n",
    "        with autocast():\n",
    "            outputs = model(gt, preds)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "            \n",
    "        # Backward pass with scaling\n",
    "        scaler.scale(loss).backward()\n",
    "    \n",
    "        if (step + 1) % CONFIG['n_accumulate'] == 0:\n",
    "            # Step the optimizer\n",
    "            scaler.step(optimizer)\n",
    "\n",
    "            # Update the scale for next iteration\n",
    "            scaler.update()\n",
    "            # optimizer.step()\n",
    "\n",
    "            # zero the parameter gradients\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            if scheduler is not None:\n",
    "                scheduler.step()\n",
    "        \n",
    "        # pdb.set_trace()\n",
    "        probabilities = torch.softmax(outputs, dim=1).detach().cpu().numpy() if outputs.shape[1] > 1 else torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        preds = np.eye(outputs.shape[1])[np.argmax(probabilities, axis=1)] if outputs.shape[1] > 1 else (probabilities > 0.5).astype(float)\n",
    "        # pdb.set_trace()\n",
    "        auroc = average_precision_score(targets.cpu().numpy(), probabilities)\n",
    "        f1 = f1_score(targets.cpu().numpy(), preds, average='weighted')\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_auroc  += (auroc * batch_size)\n",
    "        running_f1 += (f1 * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_auroc = running_auroc / dataset_size\n",
    "        epoch_f1 = running_f1 / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Train_Loss=epoch_loss, Train_Auroc=epoch_auroc, Train_F1=epoch_f1,\n",
    "                        LR=optimizer.param_groups[0]['lr'])\n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc, epoch_f1\n",
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "    running_f1 = 0.0 \n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        gt, preds, targets = data\n",
    "        gt, preds, targets = gt.to(CONFIG['device']), preds.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "        batch_size = gt.shape[0]\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(gt, preds)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1).detach().cpu().numpy() if outputs.shape[1] > 1 else torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        preds = np.eye(outputs.shape[1])[np.argmax(probabilities, axis=1)] if outputs.shape[1] > 1 else (probabilities > 0.5).astype(float)\n",
    "        \n",
    "        auroc = average_precision_score(targets.cpu().numpy(), probabilities)\n",
    "        f1 = f1_score(targets.cpu().numpy(), preds, average='weighted')\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_auroc  += (auroc * batch_size)\n",
    "        running_f1 += (f1 * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_auroc = running_auroc / dataset_size\n",
    "        epoch_f1 = running_f1 / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, Valid_Auroc=epoch_auroc, \n",
    "                        Valid_F1=epoch_f1,\n",
    "                        )   \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initializing components\n",
    "1. Model\n",
    "2. AdamW optimizer\n",
    "3. Cosine annealing scheduler\n",
    "4. Weighted cross entropy loss to handle class imbalance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SiameseNetwork().to(CONFIG['device'])\n",
    "optimizer = optim.AdamW(model.parameters(), lr=CONFIG['learning_rate'], \n",
    "                       weight_decay=CONFIG['weight_decay'])\n",
    "scheduler = lr_scheduler.CosineAnnealingLR(optimizer,T_max=CONFIG['T_max'], \n",
    "                                                   eta_min=CONFIG['min_lr'])\n",
    "print(df[['unable_to_assess','close_match', 'near_exact_match','exact_match']].sum())\n",
    "class_distribution = {'unable_to_assess': 73, \n",
    "                      'close_match': 605, \n",
    "                      'near_exact_match': 360, \n",
    "                      'exact_match': 73}\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = sum(class_distribution.values())\n",
    "class_weights = {label: total_samples / (len(class_distribution) * count) for label, count in class_distribution.items()}\n",
    "\n",
    "# Convert weights to a tensor\n",
    "weights = torch.tensor([class_weights['unable_to_assess'],\n",
    "                        class_weights['close_match'],\n",
    "                        class_weights['near_exact_match'],\n",
    "                        class_weights['exact_match']], dtype=torch.float32).to(CONFIG['device'])\n",
    "\n",
    "# Modify the loss function in your training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_one_epoch(model=model, optimizer=optimizer, criterion=criterion, scheduler=scheduler, dataloader=trainloader)\n",
    "# valid_one_epoch(model=model, dataloader=trainloader, criterion=criterion, epoch=CONFIG['epochs'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting all together into training code\n",
    "Training code includes:\n",
    "1. Early stopping\n",
    "2. Saving best model weights according to supplied name\n",
    "3. Original code adapted from Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_training(model, optimizer, scheduler, criterion, num_epochs, train_loader, valid_loader, name):\n",
    "    if torch.cuda.is_available():\n",
    "        print(\"[INFO] Using GPU: {}\\n\".format(torch.cuda.get_device_name()))\n",
    "    \n",
    "    start = time.time()\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_epoch_f1 = -np.inf\n",
    "    best_valid_loss = np.inf\n",
    "    history = defaultdict(list)\n",
    "    \n",
    "    for epoch in range(1, num_epochs + 1): \n",
    "        gc.collect()\n",
    "        train_epoch_loss, train_epoch_auroc, train_epoch_f1 = train_one_epoch(model=model, optimizer=optimizer, scheduler=scheduler, \n",
    "                                           criterion=criterion, dataloader=train_loader, \n",
    "                                           epoch=epoch)\n",
    "        \n",
    "        val_epoch_loss, val_epoch_auroc, val_epoch_f1 = valid_one_epoch(model=model, dataloader=valid_loader, criterion=criterion, \n",
    "                                         epoch=epoch)\n",
    "    \n",
    "        history['Train Loss'].append(train_epoch_loss)\n",
    "        history['Valid Loss'].append(val_epoch_loss)\n",
    "        history['Train AUROC'].append(train_epoch_auroc)\n",
    "        history['Valid AUROC'].append(val_epoch_auroc)\n",
    "        history['Valid F1'].append(val_epoch_f1)\n",
    "        history['lr'].append( scheduler.get_lr()[0] )\n",
    "        if val_epoch_loss <= best_valid_loss:\n",
    "            print(f\"Validation Loss Improved ({best_valid_loss} ---> {val_epoch_loss})\")\n",
    "            best_valid_loss = val_epoch_loss\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"best_VAL_LOSS_model_{name}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved\")\n",
    "        \n",
    "        if best_epoch_f1 <= val_epoch_f1:\n",
    "            print(f\"Validation F1 Improved ({best_epoch_f1} ---> {val_epoch_f1})\")\n",
    "            best_epoch_f1 = val_epoch_f1\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())\n",
    "            PATH = f\"best_F1_model_{name}.bin\"\n",
    "            torch.save(model.state_dict(), PATH)\n",
    "            # Save a model file from the current directory\n",
    "            print(f\"Model Saved\")\n",
    "            \n",
    "        print()\n",
    "    \n",
    "    end = time.time()\n",
    "    time_elapsed = end - start\n",
    "    print('Training complete in {:.0f}h {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 3600, (time_elapsed % 3600) // 60, (time_elapsed % 3600) % 60))\n",
    "    print(\"Best F1: {:.4f}\".format(best_epoch_f1))\n",
    "    print(\"Best Loss: {:.4f}\".format(best_valid_loss))\n",
    "    \n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    \n",
    "    return model, history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, history = run_training(model=model, optimizer=optimizer, \n",
    "                              scheduler=scheduler, criterion=criterion, \n",
    "                              num_epochs=100, train_loader=trainloader, valid_loader=validloader, \n",
    "                              name='siamese_multiclass')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metrics(metrics_dict):\n",
    "    epochs = range(1, len(metrics_dict['Train Loss']) + 1)\n",
    "    \n",
    "    # Create a figure with subplots\n",
    "    fig, axs = plt.subplots(3, 1, figsize=(10, 15), sharex=True)\n",
    "\n",
    "    # Plot Train and Validation Loss\n",
    "    axs[0].plot(epochs, metrics_dict['Train Loss'], label='Train Loss', color='blue', marker='o')\n",
    "    axs[0].plot(epochs, metrics_dict['Valid Loss'], label='Valid Loss', color='orange', marker='o')\n",
    "    axs[0].set_title('Loss Over Epochs')\n",
    "    axs[0].set_ylabel('Loss')\n",
    "    axs[0].legend()\n",
    "    axs[0].grid()\n",
    "\n",
    "    # Plot Train and Validation AUROC\n",
    "    axs[1].plot(epochs, metrics_dict['Train AUROC'], label='Train AUROC', color='green', marker='o')\n",
    "    axs[1].plot(epochs, metrics_dict['Valid AUROC'], label='Valid AUROC', color='red', marker='o')\n",
    "    axs[1].set_title('AUROC Over Epochs')\n",
    "    axs[1].set_ylabel('AUROC Score')\n",
    "    axs[1].legend()\n",
    "    axs[1].grid()\n",
    "\n",
    "    \n",
    "    # Plot Validation F1 Score\n",
    "    axs[2].plot(epochs, metrics_dict['Valid F1'], label='Valid F1 Score', color='purple', marker='o')\n",
    "    axs[2].set_title('Validation F1 Score Over Epochs')\n",
    "    axs[2].set_ylabel('F1 Score')\n",
    "    \n",
    "    \n",
    "    # Create a separate plot for Learning Rate\n",
    "    fig_lr, ax_lr = plt.subplots(figsize=(10, 5))\n",
    "    ax_lr.plot(epochs, metrics_dict['lr'], label='Learning Rate', color='cyan', linestyle='--', marker='o')\n",
    "    ax_lr.set_title('Learning Rate Over Epochs')\n",
    "    ax_lr.set_ylabel('Learning Rate')\n",
    "    ax_lr.grid()\n",
    "    fontsize = 12\n",
    "    # Label points for each plot\n",
    "    for i in range(0, len(metrics_dict['lr']), 10):\n",
    "        axs[0].text(epochs[i], metrics_dict['Train Loss'][i], f\"{metrics_dict['Train Loss'][i]:.2f}\", \n",
    "                    fontsize=fontsize, ha='right', color='k')\n",
    "        axs[0].text(epochs[i], metrics_dict['Valid Loss'][i], f\"{metrics_dict['Valid Loss'][i]:.2f}\", \n",
    "                    fontsize=fontsize, ha='right', color='k')\n",
    "        \n",
    "        axs[1].text(epochs[i], metrics_dict['Train AUROC'][i], f\"{metrics_dict['Train AUROC'][i]:.2f}\", \n",
    "                    fontsize=fontsize, ha='right', color='k')\n",
    "        axs[1].text(epochs[i], metrics_dict['Valid AUROC'][i], f\"{metrics_dict['Valid AUROC'][i]:.2f}\", \n",
    "                    fontsize=fontsize, ha='right', color='k')\n",
    "\n",
    "        axs[2].text(epochs[i], metrics_dict['Valid F1'][i], f\"{metrics_dict['Valid F1'][i]:.2f}\", \n",
    "                    fontsize=fontsize, ha='right', color='k')\n",
    "\n",
    "        ax_lr.text(epochs[i], metrics_dict['lr'][i], f\"{metrics_dict['lr'][i]:.4f}\", \n",
    "                   fontsize=fontsize, ha='right', color='k')\n",
    "\n",
    "    # Set common x-label\n",
    "    axs[-1].set_xlabel('Epochs')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # Show plots\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_metrics(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.softmax(model(gt.to(CONFIG['device']), pred.to(CONFIG['device'])), dim=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

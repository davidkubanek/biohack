{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_auc_score, average_precision_score\n",
    "import gc\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "from collections import defaultdict\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "import pdb\n",
    "\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining global parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_available_device():\n",
    "    if torch.cuda.is_available():\n",
    "        return \"cuda\"\n",
    "    elif torch.backends.mps.is_available():\n",
    "        return \"mps\"\n",
    "    else:\n",
    "        return \"cpu\"\n",
    "    \n",
    "CONFIG = {\n",
    "    \"seed\": 42,\n",
    "    \"epochs\": 12, # 42, ~MAX 20 hours of training\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    \"learning_rate\": 5e-5,\n",
    "    \"scheduler\": 'CosineAnnealingLR',\n",
    "    \"min_lr\": 5e-7,\n",
    "    \"T_max\": 12,\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"fold\" : 0,\n",
    "    \"n_fold\": 5,\n",
    "    \"n_accumulate\": 1,\n",
    "    \"device\": get_available_device(),\n",
    "    'labels': ['unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'], # 'unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'\n",
    "    'FP': 'molformer', # 'fp', 'molformer', 'ECFP', 'grover'\n",
    "}\n",
    "\n",
    "def seed_everything(seed=42):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(seed=CONFIG['seed'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Read the data that has duplicates REMOVED\n",
    "## Validation set of 20% of the size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_pickle('../data/train_split_remove_duplicates_all_embeddings.pkl')\n",
    "valid_df = pd.read_pickle('../data/valid_split_remove_duplicates_all_embeddings.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if CONFIG['FP'] == 'ECFP':\n",
    "    CONFIG['input_size'] = 2048\n",
    "elif CONFIG['FP'] == 'molformer':\n",
    "    CONFIG['input_size'] = 768\n",
    "elif CONFIG['FP'] == 'fp':\n",
    "    CONFIG['input_size'] = 2215\n",
    "elif CONFIG['FP'] == 'grover':\n",
    "    CONFIG['input_size'] = 5000\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping in Pytorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EnvedaDataset(Dataset):\n",
    "    def __init__(self, CONFIG, dataframe, labels = []):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            dataframe (pd.DataFrame): A DataFrame containing 'ground_truth_embeddings', \n",
    "                                       'predicted_embeddings', and output columns.\n",
    "        \"\"\"\n",
    "        self.dataframe = dataframe\n",
    "        # pdb.set_trace()\n",
    "        if CONFIG['FP'] == 'molformer':\n",
    "            # Convert Molformer embeddings to tensors\n",
    "            self.ground_truth_embeddings = torch.tensor(dataframe['ground_truth_embeddings'].tolist(), dtype=torch.float32)\n",
    "            self.predicted_embeddings = torch.tensor(dataframe['predicted_embeddings'].tolist(), dtype=torch.float32)\n",
    "    \n",
    "        # fingerprints\n",
    "        elif CONFIG['FP'] == 'ECFP':\n",
    "            self.ground_truth_embeddings = torch.tensor(dataframe['ground_truth_ECFP'].tolist(), dtype=torch.float32)\n",
    "            self.predicted_embeddings = torch.tensor(dataframe['predicted_ECFP'].tolist(), dtype=torch.float32)\n",
    "        elif CONFIG['FP'] == 'fp':\n",
    "            self.ground_truth_embeddings = dataframe['ground_truth_fp'].tolist()\n",
    "            self.predicted_embeddings = dataframe['predicted_fp'].tolist()\n",
    "\n",
    "        elif CONFIG['FP'] == 'grover':\n",
    "            self.ground_truth_embeddings = torch.tensor(dataframe['ground_truth_grover_fp'].tolist(), dtype=torch.float32)\n",
    "            self.predicted_embeddings = torch.tensor(dataframe['predicted_grover_fp'].tolist(), dtype=torch.float32)\n",
    "    \n",
    "        \n",
    "        self.labels_text = labels\n",
    "        # Convert labels to tensor\n",
    "        self.labels = torch.tensor(dataframe[labels].values, dtype=torch.float32)\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the total number of samples.\"\"\"\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Generates one sample of data.\"\"\"\n",
    "        if CONFIG['FP'] == 'molformer':\n",
    "            return self.ground_truth_embeddings[idx].squeeze(0), self.predicted_embeddings[idx].squeeze(0), self.labels[idx]\n",
    "        elif CONFIG['FP'] == 'fp':\n",
    "            return self.ground_truth_embeddings.iloc[idx].squeeze(0), self.predicted_embeddings.iloc[idx].squeeze(0), self.labels[idx]\n",
    "        elif CONFIG['FP'] == 'ECFP':\n",
    "            return self.ground_truth_embeddings[idx], self.predicted_embeddings[idx], self.labels[idx]\n",
    "        elif CONFIG['FP'] == 'grover':\n",
    "            return self.ground_truth_embeddings[idx].squeeze(0), self.predicted_embeddings[idx].squeeze(0), self.labels[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONFIG_ECFP = {\n",
    "    \"seed\": 42,\n",
    "    'labels': ['unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'], # 'unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    'FP': 'ECFP', # 'fp', 'molformer', 'ECFP', 'grover'\n",
    "    'input_size': 2048\n",
    "}\n",
    "\n",
    "CONFIG_molformer = {\n",
    "    \"seed\": 42,\n",
    "    'labels': ['unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'], # 'unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    'FP': 'molformer', # 'fp', 'molformer', 'ECFP', 'grover'\n",
    "    'input_size': 768\n",
    "}\n",
    "\n",
    "CONFIG_fp = {\n",
    "    \"seed\": 42,\n",
    "    'labels': ['unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'], # 'unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    'FP': 'fp',  # 'fp', 'molformer', 'ECFP', 'grover'\n",
    "    'input_size': 2215\n",
    "}\n",
    "\n",
    "CONFIG_grover = {\n",
    "    \"seed\": 42,\n",
    "    'labels': ['unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'], # 'unable_to_assess', 'not_close_match','close_match', 'near_exact_match', 'exact_match'\n",
    "    \"train_batch_size\": 16,\n",
    "    \"valid_batch_size\": 64,\n",
    "    'FP': 'grover', # 'fp', 'molformer', 'ECFP', 'grover'\n",
    "    'input_size': 5000\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize trainsets and validsets for each configuration\n",
    "trainset_ECFP = EnvedaDataset(CONFIG_ECFP, dataframe=train_df, labels=CONFIG_ECFP['labels'])\n",
    "validset_ECFP = EnvedaDataset(CONFIG_ECFP, dataframe=valid_df, labels=CONFIG_ECFP['labels'])\n",
    "\n",
    "trainset_molformer = EnvedaDataset(CONFIG_molformer, dataframe=train_df, labels=CONFIG_molformer['labels'])\n",
    "validset_molformer = EnvedaDataset(CONFIG_molformer, dataframe=valid_df, labels=CONFIG_molformer['labels'])\n",
    "\n",
    "trainset_fp = EnvedaDataset(CONFIG_fp, dataframe=train_df, labels=CONFIG_fp['labels'])\n",
    "validset_fp = EnvedaDataset(CONFIG_fp, dataframe=valid_df, labels=CONFIG_fp['labels'])\n",
    "\n",
    "trainset_grover = EnvedaDataset(CONFIG_grover, dataframe=train_df, labels=CONFIG_grover['labels'])\n",
    "validset_grover = EnvedaDataset(CONFIG_grover, dataframe=valid_df, labels=CONFIG_grover['labels'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wrapping in Pytorch DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trainloader, validloader = DataLoader(trainset, batch_size=CONFIG['train_batch_size']), \\\n",
    "#                            DataLoader(validset, batch_size=CONFIG['valid_batch_size'])\n",
    "\n",
    "\n",
    "# Initialize DataLoaders for each configuration\n",
    "trainloader_ECFP = DataLoader(trainset_ECFP, batch_size=CONFIG_ECFP['train_batch_size'])\n",
    "validloader_ECFP = DataLoader(validset_ECFP, batch_size=CONFIG_ECFP['valid_batch_size'])\n",
    "\n",
    "trainloader_molformer = DataLoader(trainset_molformer, batch_size=CONFIG_molformer['train_batch_size'])\n",
    "validloader_molformer = DataLoader(validset_molformer, batch_size=CONFIG_molformer['valid_batch_size'])\n",
    "\n",
    "trainloader_fp = DataLoader(trainset_fp, batch_size=CONFIG_fp['train_batch_size'])\n",
    "validloader_fp = DataLoader(validset_fp, batch_size=CONFIG_fp['valid_batch_size'])\n",
    "\n",
    "trainloader_grover = DataLoader(trainset_grover, batch_size=CONFIG_grover['train_batch_size'])\n",
    "validloader_grover = DataLoader(validset_grover, batch_size=CONFIG_grover['valid_batch_size'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 2215]) torch.Size([64, 2215]) torch.Size([64, 5])\n"
     ]
    }
   ],
   "source": [
    "gt, pred, labels = next(iter(validloader_fp))\n",
    "print(gt.shape, pred.shape, labels.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Siamese Network\n",
    "class SiameseNetwork(nn.Module):\n",
    "    def __init__(self, input_dim=768, output_dim=4):\n",
    "        super(SiameseNetwork, self).__init__()\n",
    "\n",
    "        self.fc = nn.Sequential(\n",
    "        nn.Linear(input_dim, 1024),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(1024, 512),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(512, 256),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2),\n",
    "        nn.Linear(256, 128),\n",
    "        nn.ReLU(inplace=True),\n",
    "        nn.Dropout(0.2))\n",
    "\n",
    "        self.fc5 = nn.Linear(128, output_dim)\n",
    "\n",
    "        #self.fc5 = nn.Linear(16 * 2, output_dim)  # Output layer for similarity judgements\n",
    "\n",
    "    def forward_one(self, x):\n",
    "        return self.fc(x)\n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        out1 = self.forward_one(input1)\n",
    "        out2 = self.forward_one(input2)\n",
    "        # pdb.set_trace()\n",
    "        # Combine both outputs by concatenation\n",
    "        # combined = torch.concat((out1, out2), dim=1) # concatenate embeddings\n",
    "        combined = torch.sub(out1, out2)  # maybe torch.abs(out1 - out2)\n",
    "        output = self.fc5(combined)                  # Outputs raw logits\n",
    "        return output\n",
    "    \n",
    "# model = SiameseNetwork()\n",
    "# model(gt, pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation regime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "@torch.no_grad()\n",
    "def valid_one_epoch(model, dataloader, criterion, epoch):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "    running_f1 = 0.0 \n",
    "    \n",
    "    bar = tqdm(enumerate(dataloader), total=len(dataloader))\n",
    "    for step, data in bar:\n",
    "        \n",
    "        gt, preds, targets = data\n",
    "        gt, preds, targets = gt.to(CONFIG['device']), preds.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "        batch_size = gt.shape[0]\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model(gt, preds)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss = loss / CONFIG['n_accumulate']\n",
    "        \n",
    "        probabilities = torch.softmax(outputs, dim=1).detach().cpu().numpy() if outputs.shape[1] > 1 else torch.sigmoid(outputs).detach().cpu().numpy()\n",
    "        preds = np.eye(outputs.shape[1])[np.argmax(probabilities, axis=1)] if outputs.shape[1] > 1 else (probabilities > 0.5).astype(float)\n",
    "        \n",
    "        auroc = average_precision_score(targets.cpu().numpy(), probabilities, average='weighted')\n",
    "        f1 = f1_score(targets.cpu().numpy(), preds, average='weighted')\n",
    "        \n",
    "        running_loss += (loss.item() * batch_size)\n",
    "        running_auroc  += (auroc * batch_size)\n",
    "        running_f1 += (f1 * batch_size)\n",
    "        dataset_size += batch_size\n",
    "        \n",
    "        epoch_loss = running_loss / dataset_size\n",
    "        epoch_auroc = running_auroc / dataset_size\n",
    "        epoch_f1 = running_f1 / dataset_size\n",
    "        \n",
    "        bar.set_postfix(Epoch=epoch, Valid_Loss=epoch_loss, Valid_Auroc=epoch_auroc, \n",
    "                        Valid_F1=epoch_f1,\n",
    "                        )   \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc, epoch_f1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize pretrained models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models for each configuration\n",
    "model_ECFP = SiameseNetwork(input_dim=CONFIG_ECFP['input_size'], output_dim=len(CONFIG_ECFP['labels'])).to(CONFIG['device'])\n",
    "model_molformer = SiameseNetwork(input_dim=CONFIG_molformer['input_size'], output_dim=len(CONFIG_molformer['labels'])).to(CONFIG['device'])\n",
    "model_fp = SiameseNetwork(input_dim=CONFIG_fp['input_size'], output_dim=len(CONFIG_fp['labels'])).to(CONFIG['device'])\n",
    "model_grover = SiameseNetwork(input_dim=CONFIG_grover['input_size'], output_dim=len(CONFIG_grover['labels'])).to(CONFIG['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the state_dict for each model\n",
    "model_ECFP.load_state_dict(torch.load('../results/best_F1_model_siamese_multiclass_sub_ECFP.bin', map_location=CONFIG['device']))\n",
    "model_molformer.load_state_dict(torch.load('../results/best_F1_model_siamese_multiclass_sub_molformer.bin', map_location=CONFIG['device']))\n",
    "model_fp.load_state_dict(torch.load('../results/best_F1_model_siamese_multiclass_sub_remove_duplicate.bin', map_location=CONFIG['device']))\n",
    "model_grover.load_state_dict(torch.load('../results/best_F1_model_siamese_multiclass_sub_grover.bin', map_location=CONFIG['device']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define global criterion for evaluating all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_distribution = {'unable_to_assess': 10,\n",
    "                      'not_close_match': 109, \n",
    "                      'close_match': 125, \n",
    "                      'near_exact_match': 66, \n",
    "                      'exact_match': 12,\n",
    "                      'good_enough_for_prioritization': 224}\n",
    "\n",
    "# Calculate class weights\n",
    "total_samples = sum(class_distribution.values())\n",
    "class_weights = {label: total_samples / (len(class_distribution) * count) for label, count in class_distribution.items()}\n",
    "\n",
    "# Convert weights to a tensor\n",
    "weights = torch.tensor([class_weights['unable_to_assess'],\n",
    "                        class_weights['not_close_match'],\n",
    "                        class_weights['close_match'],\n",
    "                        class_weights['near_exact_match'],\n",
    "                        class_weights['exact_match']], dtype=torch.float32).to(CONFIG['device'])\n",
    "\n",
    "# find the indeces of keys in class_distribution in CONFIG['labels']\n",
    "weights = torch.tensor([class_weights[c] for c in CONFIG['labels']], dtype=torch.float32).to(CONFIG['device'])\n",
    "\n",
    "# Modify the loss function in your training loop\n",
    "criterion = nn.CrossEntropyLoss(weight=weights)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance of individual models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, Epoch=1, Valid_Auroc=0.372, Valid_F1=0.319, Valid_Loss=2.89]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:00<00:00, 78.20it/s, Epoch=1, Valid_Auroc=0.358, Valid_F1=0.297, Valid_Loss=3.34]\n"
     ]
    }
   ],
   "source": [
    "# Evaluate each model individually\n",
    "val_epoch_loss_ECFP, val_epoch_auroc_ECFP, val_epoch_f1_ECFP = valid_one_epoch(\n",
    "    model=model_ECFP,\n",
    "    dataloader=validloader_ECFP,\n",
    "    criterion=criterion,\n",
    "    epoch=1\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, Epoch=1, Valid_Auroc=0.422, Valid_F1=0.428, Valid_Loss=1.86]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:00<00:00, 93.68it/s, Epoch=1, Valid_Auroc=0.394, Valid_F1=0.403, Valid_Loss=2.96]\n"
     ]
    }
   ],
   "source": [
    "val_epoch_loss_molformer, val_epoch_auroc_molformer, val_epoch_f1_molformer = valid_one_epoch(\n",
    "    model=model_molformer,\n",
    "    dataloader=validloader_molformer,\n",
    "    criterion=criterion,\n",
    "    epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, Epoch=1, Valid_Auroc=0.353, Valid_F1=0.348, Valid_Loss=2.13]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:00<00:00, 97.74it/s, Epoch=1, Valid_Auroc=0.359, Valid_F1=0.352, Valid_Loss=2.16]\n"
     ]
    }
   ],
   "source": [
    "val_epoch_loss_fp, val_epoch_auroc_fp, val_epoch_f1_fp = valid_one_epoch(\n",
    "    model=model_fp,\n",
    "    dataloader=validloader_fp,\n",
    "    criterion=criterion,\n",
    "    epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/2 [00:00<?, ?it/s]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "  0%|          | 0/2 [00:00<?, ?it/s, Epoch=1, Valid_Auroc=0.389, Valid_F1=0.313, Valid_Loss=1.98]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/sklearn/metrics/_ranking.py:980: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "100%|██████████| 2/2 [00:00<00:00, 44.20it/s, Epoch=1, Valid_Auroc=0.425, Valid_F1=0.328, Valid_Loss=1.84]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "val_epoch_loss_grover, val_epoch_auroc_grover, val_epoch_f1_grover = valid_one_epoch(\n",
    "    model=model_grover,\n",
    "    dataloader=validloader_grover,\n",
    "    criterion=criterion,\n",
    "    epoch=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Ensemble of Siamese network with different feature encoders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Model ecfp:   0%|          | 0/2 [00:00<?, ?it/s]/Users/awxlong/anaconda3/envs/ai/lib/python3.8/site-packages/torch/amp/autocast_mode.py:250: UserWarning: User provided device_type of 'cuda', but CUDA is not available. Disabling\n",
      "  warnings.warn(\n",
      "Model ecfp: 100%|██████████| 2/2 [00:00<00:00, 179.53it/s]\n",
      "Model molformer: 100%|██████████| 2/2 [00:00<00:00, 385.65it/s]\n",
      "Model fp: 100%|██████████| 2/2 [00:00<00:00, 398.40it/s]\n",
      "Model grover: 100%|██████████| 2/2 [00:00<00:00, 67.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Valid Loss: 2.067840814590454, Valid AUROC: 0.366906338762862, Valid F1: 0.3713195435584391\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "@torch.no_grad()\n",
    "def valid_ensemble(models, dataloaders, criterion, epoch):\n",
    "    # Set models to evaluation mode\n",
    "    for model in models:\n",
    "        model.eval()\n",
    "    \n",
    "    dataset_size = 0\n",
    "    running_loss = 0.0\n",
    "    running_auroc = 0.0\n",
    "    running_f1 = 0.0 \n",
    "\n",
    "    # Initialize a list to accumulate model outputs\n",
    "    all_outputs = []\n",
    "\n",
    "    # Initialize a tensor for targets (assuming they are consistent across all dataloaders)\n",
    "    targets_list = []\n",
    "    model_list_names = ['ecfp', 'molformer', 'fp', 'grover']\n",
    "    # Iterate over each dataloader and evaluate the corresponding model\n",
    "    for i, (dataloader, model) in enumerate(zip(dataloaders, models)):\n",
    "        bar = tqdm(enumerate(dataloader), total=len(dataloader), desc=f'Model {model_list_names[i]}')\n",
    "        \n",
    "        for step, data in bar:\n",
    "            gt, preds, targets = data  # Get targets from each batch\n",
    "            gt, preds, targets = gt.to(CONFIG['device']), preds.to(CONFIG['device']), targets.to(CONFIG['device'])\n",
    "            \n",
    "            with autocast():\n",
    "                outputs = model(gt, preds)\n",
    "                all_outputs.append(outputs)  # Store outputs on CPU for later averaging\n",
    "            \n",
    "            if i == 0:  # Only collect targets from the first dataloader\n",
    "                targets_list.append(targets)\n",
    "\n",
    "    # After processing all batches for all models, concatenate the outputs\n",
    "    all_outputs_tensor = torch.cat(all_outputs, dim=0)  # Shape will be (total_samples, 5)\n",
    "\n",
    "    # Average the outputs from all models\n",
    "    ensemble_outputs = torch.mean(all_outputs_tensor.view(len(models), -1, 5), dim=0)  # Shape should be (88, 5)\n",
    "\n",
    "    # Concatenate targets into a single tensor (assuming they are consistent)\n",
    "    targets_tensor = torch.cat(targets_list, dim=0)  # Shape should be (88, 5)\n",
    "\n",
    "    # Calculate loss and metrics using the ensemble outputs\n",
    "    loss = criterion(ensemble_outputs, targets_tensor)\n",
    "\n",
    "    probabilities = torch.softmax(ensemble_outputs, dim=1).detach().cpu().numpy() if ensemble_outputs.shape[1] > 1 else torch.sigmoid(ensemble_outputs).detach().cpu().numpy()\n",
    "    preds = np.eye(ensemble_outputs.shape[1])[np.argmax(probabilities, axis=1)] if ensemble_outputs.shape[1] > 1 else (probabilities > 0.5).astype(float)\n",
    "\n",
    "    auroc = average_precision_score(targets_tensor.cpu().numpy(), probabilities, average='weighted')\n",
    "    f1 = f1_score(targets_tensor.cpu().numpy(), preds, average='weighted')\n",
    "\n",
    "    running_loss += loss.item() * len(targets_tensor)\n",
    "    running_auroc += auroc * len(targets_tensor)\n",
    "    running_f1 += f1 * len(targets_tensor)\n",
    "    dataset_size += len(targets_tensor)\n",
    "\n",
    "    epoch_loss = running_loss / dataset_size\n",
    "    epoch_auroc = running_auroc / dataset_size\n",
    "    epoch_f1 = running_f1 / dataset_size\n",
    "\n",
    "    print(f'Epoch: {epoch}, Valid Loss: {epoch_loss}, Valid AUROC: {epoch_auroc}, Valid F1: {epoch_f1}')\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return epoch_loss, epoch_auroc, epoch_f1\n",
    "\n",
    "# Example usage:\n",
    "models = [model_ECFP, model_molformer, model_fp, model_grover]\n",
    "dataloaders = [validloader_ECFP, validloader_molformer, validloader_fp, validloader_grover]\n",
    "\n",
    "# Call the ensemble evaluation function for a specific epoch\n",
    "loss, auroc, f1 = valid_ensemble(models, dataloaders, criterion, 1)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DTMS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
